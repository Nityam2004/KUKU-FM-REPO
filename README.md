# ğŸ™ï¸ KUKU-FM-REPO: Script-to-Voice

**AI-Powered Script to Video Pipeline**

A fully automated, end-to-end system that converts raw text scripts or podcast transcripts into emotionally expressive, subtitle-synced videos â€” complete with AI-generated visuals and natural voiceovers.

---

## ğŸ” Overview

**Script-to-Voice** streamlines multimedia content creation by converting textual narratives into complete videos. It combines emotion-aware speech synthesis, AI-based visual scene generation, and automated video assembly for scalable content production.

---

## ğŸš€ Features

- ğŸ­ **Emotion-Aware Voice Modulation**  
  SSML-tuned Google WaveNet voices adapt tone and pitch based on emotional cues.

- ğŸ§  **Transformer-Based Emotion Detection**  
  Classifies emotions (happy, sad, angry, etc.) using models like BERT.

- ğŸ¨ **Scene Generation with AI**  
  Generates illustrations from descriptive prompts using OpenAI's DALLÂ·E.

- ğŸ—£ï¸ **Natural Speech Synthesis**  
  Uses Google Cloud Text-to-Speech API with 220+ WaveNet voices across 40+ languages.

- ğŸï¸ **Subtitle Embedding & Video Rendering**  
  Subtitles generated from tokenized text and rendered with audio and visuals using FFmpeg.

- â˜ï¸ **Cloud-Ready & Scalable**  
  Designed for high-throughput processing, handling 100+ scripts using cloud APIs.

---

## ğŸ”§ Pipeline Workflow

### 1. ğŸ“ Input Handling
- Accepts raw story scripts or transcribed podcasts.
- **Text Sanitization**: Cleans punctuation, removes special characters.
- **Tokenization**: Breaks content into sentences or words (using NLTK or SpaCy).
- **SSML Formatting**: Prepares the text for voice synthesis compatibility.

### 2. ğŸ˜ƒ Emotion Detection
- Employs BERT-based classifiers to detect sentence-level emotions.
- Trained on emotion-tagged datasets for accuracy.
- Output guides voice modulation during speech synthesis.

### 3. ğŸ”Š Voice Synthesis (Google WaveNet)
- Converts script to speech using emotion-informed SSML.
- Example mappings:
  - Happy â†’ Fast, high-pitch tone
  - Sad â†’ Slower, low-pitch
  - Angry â†’ Sharp, tense delivery

### 4. ğŸ–¼ï¸ Visual Generation
- Generates scenes from descriptive prompts using OpenAIâ€™s DALLÂ·E.
- Ensures visual continuity for storytelling.
- Example: `"stormy sea"` â†’ creates an appropriate narrative frame.

### 5. ğŸ¬ Video Assembly
- Combines generated images and synthesized audio using FFmpeg.
- Subtitles embedded from tokenized scripts (.srt format).
- Maintains tight audio-visual sync (<50ms drift).

---

## ğŸ§° Tech Stack

| Component          | Tools Used                                  |
|-------------------|----------------------------------------------|
| Language           | Python                                       |
| NLP & Emotion      | SpaCy / NLTK, BERT                           |
| Speech Synthesis   | Google Cloud Text-to-Speech (WaveNet + SSML)|
| Visual Generation  | OpenAI DALLÂ·E API                            |
| Video Rendering    | FFmpeg                                       |
| Deployment         | Cloud-based (Google Cloud APIs)              |

---

## âš™ï¸ Performance & Cost

- â±ï¸ **Speed**: Renders a 10-minute story in under 1 minute  
- ğŸ’° **Cost**: ~$0.016 per 1M characters (Google WaveNet pricing)  
- â˜ï¸ **Scalability**: Batch-processing ready â€” can handle 100+ scripts in parallel

---

## ğŸ’¡ Final Pitch

> **Stop hiring voice actors or spending hours editing!**  
> Feed your script into our AI, choose a voice (or even clone your own), and generate studio-quality content in seconds.  
> Perfect for scaling podcasts, storytelling, educational material, or corporate videos â€” without sacrificing emotional nuance or quality.

---

## ğŸ“œ License

[MIT License](LICENSE)

---

## ğŸ§ª Python Version

Tested with **Python 3.8+**
